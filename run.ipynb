{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0837654d9b870ae526dfaa940565f97968217b3b1f96a8d2306a1802604aaebe8",
   "display_name": "Python 3.8.8 64-bit ('python3.8': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요코드\n",
    "!pip install jupyter\n",
    "!pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting efficientnet_pytorch\n  Using cached efficientnet_pytorch-0.7.0-py3-none-any.whl\nRequirement already satisfied: torch in c:\\users\\toz\\anaconda3\\envs\\python3.8\\lib\\site-packages (from efficientnet_pytorch) (1.8.1)\nRequirement already satisfied: typing-extensions in c:\\users\\toz\\anaconda3\\envs\\python3.8\\lib\\site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\nRequirement already satisfied: numpy in c:\\users\\toz\\anaconda3\\envs\\python3.8\\lib\\site-packages (from torch->efficientnet_pytorch) (1.19.5)\nInstalling collected packages: efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer와 초기화 설정\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed 설정\n",
    "seed = 719\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 설정\n",
    "batch_size = 64  # 학습 배치 크기\n",
    "test_batch_size = 1000  # 테스트 배치 크기 (학습 과정을 제외하므로 더 큰 배치 사용 가능)\n",
    "max_epochs = 10  # 학습 데이터셋 총 훈련 횟수\n",
    "lr = 0.01  # 학습률\n",
    "momentum = 0.5  # SGD에 사용할 모멘텀 설정 (파라미터 업데이트 시 관성 효과 사용)\n",
    "log_interval = 200  # interval 때마다 로그 남김\n",
    "use_cuda = torch.cuda.is_available()  # GPU cuda 사용 여부 확인\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")  # GPU cuda 사용하거나 없다면 CPU 사용\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}  # num_workers: data loading할 프로세스 수, pin_memory: 고정된 메모리 영역 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "#모델 및 활성화 함수 세팅\n",
    "model = EfficientNet.from_pretrained('efficientnet-b4', num_classes=15).to(device)\n",
    "base_optimizer = optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=lr, momentum=momentum)  # 최적화 알고리즘 정의 (SGD와 SAM사용)\n",
    "criterion = nn.CrossEntropyLoss()  # 손실 함수 정의 (CrossEntropy 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 로더 설정\n",
    "class Custom_Dataloder(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        one_hot = {\n",
    "            'Anger'   : [1,0,0,0,0],\n",
    "            'Disgust' : [0,1,0,0,0],\n",
    "            'Fear'    : [0,0,1,0,0],\n",
    "            'Joy'     : [0,0,0,1,0],\n",
    "            'Sadness' : [0,0,0,0,1],\n",
    "        }\n",
    "\n",
    "        root_path =r'face_detecting_data\\crawler\\imageset'\n",
    "        imageset_list = os.listdir(root_path)\n",
    "        train_dataset = []\n",
    "        for emotion in imageset_list:\n",
    "            emotion_image_path = os.path.join(root_path, emotion)\n",
    "            emotion_images = os.listdir(emotion_image_path)\n",
    "            for image in emotion_images:\n",
    "                image = cv2.imread(os.path.join(emotion_image_path, image))\n",
    "                # resize\n",
    "                image = cv2.resize(image, (64,64))\n",
    "                # BGR2Gray\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                # normalize\n",
    "                image = (image[:,:] - 127.5) / 127.5\n",
    "\n",
    "                train_dataset.append([image, one_hot[emotion]])\n",
    "        random.shuffle(train_dataset)\n",
    "        train_dataset = np.array(train_dataset)\n",
    "        \n",
    "        self.x_data = train_dataset[:,0]\n",
    "        self.y_data = train_dataset[:,1]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()  # 모델 학습 모드 설정\n",
    "    summary_loss = AverageMeter()  # 학습 손실값 기록 초기화\n",
    "    summary_acc = AverageMeter() # 학습 정확도 기록 초기화\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)  # 현재 미니 배치의 데이터, 정답 불러옴\n",
    "        optimizer.zero_grad()  # gradient 0으로 초기화\n",
    "        output = model(data)  # 모델에 입력값 feed-forward\n",
    "        loss = criterion(output, target)  # 예측값(클래스 별 score)과 정답간의 손실값 계산\n",
    "        loss.backward()  # 손실값 역전파 (각 계층에서 gradient 계산, pytorch는 autograd로 gradient 자동 계산)\n",
    "        \n",
    "        # SAM 내용 추가\n",
    "        optimizer.first_step(zero_grad=True)  # 모델의 파라미터 업데이트 (gradient 이용하여 파라미터 업데이트)\n",
    "        criterion(model(data), target).backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "\n",
    "\n",
    "        summary_loss.update(loss.detach().item())  # 손실값 기록\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # 예측값 중에서 최고 score를 달성한 클래스 선발\n",
    "        correct = pred.eq(target.view_as(pred)).sum().item()  # 정답과 예측 클래스가 일치한 개수\n",
    "        summary_acc.update(correct / data.size(0))  # 정확도 기록\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}, Accuracy: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), summary_loss.avg, summary_acc.avg))\n",
    "            \n",
    "    return summary_loss.avg, summary_acc.avg\n",
    "\n",
    "def test(log_interval, model, device, test_loader):\n",
    "    model.eval()  # 모델 검증 모드 설정 (inference mode)\n",
    "    summary_loss = AverageMeter()  # 테스트 손실값 기록 초기화\n",
    "    summary_acc = AverageMeter() # 테스트 정확도 기록 초기화\n",
    "    with torch.no_grad():  # 검증 모드이므로 gradient 계산안함\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # 현재 미니 배치의 데이터, 정답 불러옴\n",
    "            output = model(data)  # 모델에 입력값 feed-forward\n",
    "            loss = criterion(output, target)  # 예측값(클래스 별 score)과 정답간의 손실값 계산\n",
    "            summary_loss.update(loss.detach().item())  # 손실값 기록\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 예측값 중에서 최고 score를 달성한 클래스 선발\n",
    "            correct = pred.eq(target.view_as(pred)).sum().item()  # 정답과 예측 클래스가 일치한 개수\n",
    "            summary_acc.update(correct / data.size(0))  # 정확도 기록\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.6f}\\n'.format\n",
    "          (summary_loss.avg, summary_acc.avg))  # 정답을 맞춘 개수 / 테스트셋 샘플 수 -> Accuracy\n",
    "\n",
    "    return summary_loss.avg, summary_acc.avg"
   ]
  }
 ]
}